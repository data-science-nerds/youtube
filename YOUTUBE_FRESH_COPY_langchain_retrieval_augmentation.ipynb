{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0Qrii3fJYbS"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-augmentation.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-augmentation.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQRA1HWOJYbU"
      },
      "source": [
        "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
        "\n",
        "# Retrieval Augmentation\n",
        "\n",
        "**L**arge **L**anguage **M**odels (LLMs) have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n",
        "\n",
        "The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n",
        "\n",
        "A solution to this problem is *retrieval augmentation*. The idea behind this is that we retrieve relevant information from an external knowledge base and give that information to our LLM. In this notebook we will learn how to do that.\n",
        "\n",
        "[![Open full notebook](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/full-link.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-augmentation.ipynb)\n",
        "\n",
        "To begin, we must install the prerequisite libraries that we will be using in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0_4wHAWtmAvJ"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  langchain==0.0.162 \\\n",
        "  openai==0.27.7 \\\n",
        "  tiktoken==0.4.0 \\\n",
        "  \"pinecone-client[grpc]\"==2.2.1 \\\n",
        "  pinecone_datasets=='0.5.0rc10'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6mmuvgpVgQD"
      },
      "source": [
        "---\n",
        "\n",
        "ðŸš¨ _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaVQA7J0JYbV"
      },
      "source": [
        "## Building the Knowledge Base\n",
        "\n",
        "We will download a pre-embedding dataset from `pinecone-datasets`. Allowing us to skip the embedding and preprocessing steps, if you'd rather work through those steps you can find the [full notebook here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-augmentation.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LeAmSrjvKJrV"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>values</th>\n",
              "      <th>sparse_values</th>\n",
              "      <th>metadata</th>\n",
              "      <th>blob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1-0</td>\n",
              "      <td>[-0.011254455894231796, -0.01698738895356655, ...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 0, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1-1</td>\n",
              "      <td>[-0.0015197008615359664, -0.007858820259571075...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 1, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1-2</td>\n",
              "      <td>[-0.009930099360644817, -0.012211072258651257,...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 2, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1-3</td>\n",
              "      <td>[-0.011600767262279987, -0.012608098797500134,...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 3, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1-4</td>\n",
              "      <td>[-0.026462381705641747, -0.016362832859158516,...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 4, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                             values sparse_values  \\\n",
              "0  1-0  [-0.011254455894231796, -0.01698738895356655, ...          None   \n",
              "1  1-1  [-0.0015197008615359664, -0.007858820259571075...          None   \n",
              "2  1-2  [-0.009930099360644817, -0.012211072258651257,...          None   \n",
              "3  1-3  [-0.011600767262279987, -0.012608098797500134,...          None   \n",
              "4  1-4  [-0.026462381705641747, -0.016362832859158516,...          None   \n",
              "\n",
              "  metadata                                               blob  \n",
              "0     None  {'chunk': 0, 'source': 'https://simple.wikiped...  \n",
              "1     None  {'chunk': 1, 'source': 'https://simple.wikiped...  \n",
              "2     None  {'chunk': 2, 'source': 'https://simple.wikiped...  \n",
              "3     None  {'chunk': 3, 'source': 'https://simple.wikiped...  \n",
              "4     None  {'chunk': 4, 'source': 'https://simple.wikiped...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pinecone_datasets\n",
        "\n",
        "dataset = pinecone_datasets.load_dataset('wikipedia-simple-text-embedding-ada-002-100K')\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_WXZkwUbKuXK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)\n",
        "# type(dataset)\n",
        "# dataset.__\n",
        "# glob = dataset[\"blob\"]\n",
        "# glob.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>values</th>\n",
              "      <th>sparse_values</th>\n",
              "      <th>metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1-0</td>\n",
              "      <td>[-0.011254455894231796, -0.01698738895356655, ...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 0, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1-1</td>\n",
              "      <td>[-0.0015197008615359664, -0.007858820259571075...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 1, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1-2</td>\n",
              "      <td>[-0.009930099360644817, -0.012211072258651257,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 2, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1-3</td>\n",
              "      <td>[-0.011600767262279987, -0.012608098797500134,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 3, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1-4</td>\n",
              "      <td>[-0.026462381705641747, -0.016362832859158516,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 4, 'source': 'https://simple.wikiped...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                             values sparse_values  \\\n",
              "0  1-0  [-0.011254455894231796, -0.01698738895356655, ...          None   \n",
              "1  1-1  [-0.0015197008615359664, -0.007858820259571075...          None   \n",
              "2  1-2  [-0.009930099360644817, -0.012211072258651257,...          None   \n",
              "3  1-3  [-0.011600767262279987, -0.012608098797500134,...          None   \n",
              "4  1-4  [-0.026462381705641747, -0.016362832859158516,...          None   \n",
              "\n",
              "                                            metadata  \n",
              "0  {'chunk': 0, 'source': 'https://simple.wikiped...  \n",
              "1  {'chunk': 1, 'source': 'https://simple.wikiped...  \n",
              "2  {'chunk': 2, 'source': 'https://simple.wikiped...  \n",
              "3  {'chunk': 3, 'source': 'https://simple.wikiped...  \n",
              "4  {'chunk': 4, 'source': 'https://simple.wikiped...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we drop sparse_values as they are not needed for this example\n",
        "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
        "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
        "# we will use rows of the dataset up to index 30_000\n",
        "dataset.documents.drop(dataset.documents.index[2000:], inplace=True)\n",
        "len(dataset)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDjJHsHUKv28"
      },
      "source": [
        "We'll format the dataset ready for upsert and reduce what we use to a subset of the full dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bmUgxFlQK1Ow"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Convert the Dataset object to a DataFrame to ensure we have a metadata column\n",
        "df = dataset.documents\n",
        "\n",
        "# 2. Modify the DataFrame\n",
        "# Assuming you want to copy the 'metadata' column to 'blob' (and create 'blob' if it doesn't exist)\n",
        "if 'blob' in df.columns:\n",
        "    df['metadata'] = df['blob']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "# Create the documents directory if it doesn't exist\n",
        "documents_dir = \"../data/processed/documents\"\n",
        "os.makedirs(documents_dir, exist_ok=True)\n",
        "\n",
        "# Save the DataFrame as a Parquet file inside the documents directory\n",
        "parquet_file_inside_documents = os.path.join(documents_dir, \"parquet_df_with_metadata.parquet\")\n",
        "df.to_parquet(parquet_file_inside_documents)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>values</th>\n",
              "      <th>sparse_values</th>\n",
              "      <th>metadata</th>\n",
              "      <th>blob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1-0</td>\n",
              "      <td>[-0.011254455894231796, -0.01698738895356655, ...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 0, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1-1</td>\n",
              "      <td>[-0.0015197008615359664, -0.007858820259571075...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 1, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1-2</td>\n",
              "      <td>[-0.009930099360644817, -0.012211072258651257,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 2, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1-3</td>\n",
              "      <td>[-0.011600767262279987, -0.012608098797500134,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 3, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1-4</td>\n",
              "      <td>[-0.026462381705641747, -0.016362832859158516,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 4, 'source': 'https://simple.wikiped...</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                             values sparse_values  \\\n",
              "0  1-0  [-0.011254455894231796, -0.01698738895356655, ...          None   \n",
              "1  1-1  [-0.0015197008615359664, -0.007858820259571075...          None   \n",
              "2  1-2  [-0.009930099360644817, -0.012211072258651257,...          None   \n",
              "3  1-3  [-0.011600767262279987, -0.012608098797500134,...          None   \n",
              "4  1-4  [-0.026462381705641747, -0.016362832859158516,...          None   \n",
              "\n",
              "                                            metadata  blob  \n",
              "0  {'chunk': 0, 'source': 'https://simple.wikiped...  None  \n",
              "1  {'chunk': 1, 'source': 'https://simple.wikiped...  None  \n",
              "2  {'chunk': 2, 'source': 'https://simple.wikiped...  None  \n",
              "3  {'chunk': 3, 'source': 'https://simple.wikiped...  None  \n",
              "4  {'chunk': 4, 'source': 'https://simple.wikiped...  None  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_dataset = pinecone_datasets.dataset.Dataset.from_path(\"../data/processed/\")\n",
        "new_dataset.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPUmWYSA43eC"
      },
      "source": [
        "Now we move on to initializing our Pinecone vector database.\n",
        "\n",
        "## Vector Database\n",
        "\n",
        "To create our vector database we first need a [free API key from Pinecone](https://app.pinecone.io). Then we initialize like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fTPKhI8TVgQF",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "index_name = 'langchain-retrieval-augmentation-fast'\n",
        "indexname = index_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9pT9C4nW4vwo"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv,find_dotenv\n",
        "load_dotenv(find_dotenv())\n",
        "import os\n",
        "import pinecone\n",
        "\n",
        "# # find API key in console at app.pinecone.io\n",
        "# PINECONE_API_KEY = os.getenv('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
        "# # find ENV (cloud region) next to API key in console\n",
        "# PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT') or 'PINECONE_ENVIRONMENT'\n",
        "\n",
        "# pinecone.init(\n",
        "#     api_key=PINECONE_API_KEY,\n",
        "#     environment=PINECONE_ENVIRONMENT\n",
        "# )\n",
        "# connect to pinecone environment\n",
        "pinecone.init(\n",
        "    api_key=os.getenv('PINECONE_API_KEY'),  \n",
        "    environment=os.getenv('PINECONE_ENV') \n",
        ")\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # we create a new index\n",
        "    pinecone.create_index(\n",
        "        name=index_name,\n",
        "        metric='cosine',\n",
        "        dimension=1536,  # 1536 dim of text-embedding-ada-002\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgPUwd6REY6z"
      },
      "source": [
        "Then we connect to the new index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RFydARw4EcoQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import time\n",
        "# allows notebook to work\n",
        "import requests\n",
        "from requests.packages.urllib3.util.ssl_ import create_urllib3_context\n",
        "\n",
        "CIPHERS = (\n",
        "    'ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20:ECDH+AESGCM:ECDH+CHACHA20:DH+AESGCM:DH+CHACHA20:'\n",
        "    'ECDHE+AES:!aNULL:!eNULL:!EXPORT:!DES:!MD5:!PSK:!RC4:!HMAC_SHA1:!SHA1:!DHE+AES:!ECDH+AES:!DH+AES'\n",
        ")\n",
        "\n",
        "requests.packages.urllib3.util.ssl_.DEFAULT_CIPHERS = CIPHERS\n",
        "# Skip the following two lines if they cause errors\n",
        "# requests.packages.urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST = CIPHERS\n",
        "# requests.packages.urllib3.contrib.pyopenssl.inject_into_urllib3()\n",
        "requests.packages.urllib3.util.ssl_.create_default_context = create_urllib3_context\n",
        "\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "# wait a moment for the index to be fully initialized\n",
        "time.sleep(1)\n",
        "\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RqIF2mIDwFu"
      },
      "source": [
        "We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
        "\n",
        "Now we upsert the data to Pinecone:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "W-cIOoTWGY1R"
      },
      "outputs": [],
      "source": [
        "for batch in dataset.iter_documents(batch_size=100):\n",
        "    index.upsert(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaF3daSxyCwB"
      },
      "source": [
        "We've now indexed everything. We can check the number of vectors in our index like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaEBhsAM22M3"
      },
      "outputs": [],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8P2PryCy8W3"
      },
      "source": [
        "## Creating a Vector Store and Querying\n",
        "\n",
        "Now that we've build our index we can switch over to LangChain. We need to initialize a LangChain vector store using the same index we just built. For this we will also need a LangChain embedding object, which we initialize like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FvwwQA4qbcK9"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "# get openai api key from platform.openai.com\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n",
        "\n",
        "model_name = 'text-embedding-ada-002'\n",
        "\n",
        "embed = OpenAIEmbeddings(\n",
        "    model=model_name,\n",
        "    openai_api_key=OPENAI_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKuedXN8bcfA"
      },
      "source": [
        "Now initialize the vector store:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qMXlvXOAyJHy"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = \"text\"\n",
        "\n",
        "# switch back to normal index for langchain\n",
        "index = pinecone.Index(index_name)\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed.embed_query, text_field\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1Yg5mKse1bO"
      },
      "source": [
        "Now we can query the vector store directly using `vectorstore.similarity_search`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "COT5s7hcyPiq"
      },
      "outputs": [
        {
          "ename": "ApiAttributeError",
          "evalue": "ScoredVector has no attribute 'metadata' at ['['received_data', 'matches', 0]']['metadata']",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mApiAttributeError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwho was Benito Mussolini?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m vectorstore\u001b[39m.\u001b[39;49msimilarity_search(\n\u001b[1;32m      4\u001b[0m     query,  \u001b[39m# our search query\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m     k\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m  \u001b[39m# return 3 most relevant docs\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/CODE/sba/sba-venv/lib/python3.11/site-packages/langchain/vectorstores/pinecone.py:158\u001b[0m, in \u001b[0;36mPinecone.similarity_search\u001b[0;34m(self, query, k, filter, namespace, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index\u001b[39m.\u001b[39mquery(\n\u001b[1;32m    151\u001b[0m     [query_obj],\n\u001b[1;32m    152\u001b[0m     top_k\u001b[39m=\u001b[39mk,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39mfilter\u001b[39m\u001b[39m=\u001b[39m\u001b[39mfilter\u001b[39m,\n\u001b[1;32m    156\u001b[0m )\n\u001b[1;32m    157\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results[\u001b[39m\"\u001b[39m\u001b[39mmatches\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 158\u001b[0m     metadata \u001b[39m=\u001b[39m res[\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m    159\u001b[0m     text \u001b[39m=\u001b[39m metadata\u001b[39m.\u001b[39mpop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text_key)\n\u001b[1;32m    160\u001b[0m     docs\u001b[39m.\u001b[39mappend(Document(page_content\u001b[39m=\u001b[39mtext, metadata\u001b[39m=\u001b[39mmetadata))\n",
            "File \u001b[0;32m~/Documents/CODE/sba/sba-venv/lib/python3.11/site-packages/pinecone/core/client/model_utils.py:502\u001b[0m, in \u001b[0;36mModelNormal.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m    500\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget(name)\n\u001b[0;32m--> 502\u001b[0m \u001b[39mraise\u001b[39;00m ApiAttributeError(\n\u001b[1;32m    503\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    504\u001b[0m         \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name),\n\u001b[1;32m    505\u001b[0m     [e \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path_to_item, name] \u001b[39mif\u001b[39;00m e]\n\u001b[1;32m    506\u001b[0m )\n",
            "\u001b[0;31mApiAttributeError\u001b[0m: ScoredVector has no attribute 'metadata' at ['['received_data', 'matches', 0]']['metadata']"
          ]
        }
      ],
      "source": [
        "query = \"who was Benito Mussolini?\"\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,  # our search query\n",
        "    k=3  # return 3 most relevant docs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCvtmREd0pdo"
      },
      "source": [
        "All of these are good, relevant results. But what can we do with this? There are many tasks, one of the most interesting (and well supported by LangChain) is called _\"Generative Question-Answering\"_ or GQA.\n",
        "\n",
        "## Generative Question-Answering\n",
        "\n",
        "In GQA we take the query as a question that is to be answered by a LLM, but the LLM must answer the question based on the information it is seeing being returned from the `vectorstore`.\n",
        "\n",
        "To do this we initialize a `RetrievalQA` object like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moCvQR-p0Zsb"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# completion llm\n",
        "llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KS9sa19K3LkQ"
      },
      "outputs": [],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qf5e3xf3ggq"
      },
      "source": [
        "We can also include the sources of information that the LLM is using to answer our question. We can do this using a slightly different version of `RetrievalQA` called `RetrievalQAWithSourcesChain`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYVMGDA13cTz"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXsVEh3S4ZJO"
      },
      "outputs": [],
      "source": [
        "qa_with_sources(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRk_P3Q7l5J"
      },
      "source": [
        "Now we answer the question being asked, *and* return the source of this information being used by the LLM.\n",
        "\n",
        "Once done, we can delete the index to save resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNkoIzQjjMTQ"
      },
      "outputs": [],
      "source": [
        "# pinecone.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehJEn68qADoH"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
